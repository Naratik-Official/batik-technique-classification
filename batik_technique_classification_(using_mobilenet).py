# -*- coding: utf-8 -*-
"""batik_technique_classification_(using_mobilenet).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17XsyEXUvN_nWIdg0eJjKl6bcbD2a4HYL
"""

from google.colab import drive
drive.mount("/content/gdrive")

from google.colab import files
from keras_preprocessing import image
from shutil import copyfile
import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.applications.mobilenet_v2 import preprocess_input
from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Activation, Dense, Flatten, Conv2D, MaxPooling2D, Dropout
from tensorflow.keras.optimizers import RMSprop
import matplotlib.pyplot as plt
import numpy as np
import os
import random

print("TensorFlow Version :", tf.__version__)

# print("Farrel")
# farrel_traditional_source_dir = '/content/gdrive/Shared drives/Capstone Project/Dataset/technique-based dataset/1-farrel/traditional/'
# farrel_modern_source_dir = '/content/gdrive/Shared drives/Capstone Project/Dataset/technique-based dataset/1-farrel/modern/'
# print('Traditional Batik:', len(os.listdir(farrel_traditional_source_dir)))
# print('Modern Batik:', len(os.listdir(farrel_modern_source_dir)))

# print("\nDosen")
# dosen_traditional_source_dir = '/content/gdrive/Shared drives/Capstone Project/Dataset/technique-based dataset/2-dosen/traditional/'
# dosen_modern_source_dir = '/content/gdrive/Shared drives/Capstone Project/Dataset/technique-based dataset/2-dosen/modern/'
# print('Traditional Batik:', len(os.listdir(dosen_traditional_source_dir)))
# print('Modern Batik:', len(os.listdir(dosen_modern_source_dir)))

# print("\nAnggun")
# anggun_traditional_source_dir = '/content/gdrive/Shared drives/Capstone Project/Dataset/technique-based dataset/3-anggun/traditional/'
# anggun_modern_source_dir = '/content/gdrive/Shared drives/Capstone Project/Dataset/technique-based dataset/3-anggun/modern/'
# print('Traditional Batik:', len(os.listdir(anggun_traditional_source_dir)))
# print('Modern Batik:', len(os.listdir(anggun_modern_source_dir)))

# print("\nRiqqah")
# riqqah_traditional_source_dir = '/content/gdrive/Shared drives/Capstone Project/Dataset/technique-based dataset/4-riqqah/traditional/'
# riqqah_modern_source_dir = '/content/gdrive/Shared drives/Capstone Project/Dataset/technique-based dataset/4-riqqah/modern/'
# print('Traditional Batik:', len(os.listdir(riqqah_traditional_source_dir)))
# print('Modern Batik:', len(os.listdir(riqqah_modern_source_dir)))

# def copy_data(SOURCE, DESTINATION):
#   file_names = os.listdir(SOURCE)

#   for index, file in enumerate(file_names):
#     copyfile(os.path.join(SOURCE, file), os.path.join(DESTINATION, file))

# gdrive_traditional_dir = '/content/gdrive/Shared drives/Capstone Project/Dataset/technique-based dataset/all-v2/traditional/'
# gdrive_modern_dir = '/content/gdrive/Shared drives/Capstone Project/Dataset/technique-based dataset/all-v2/modern/'

# copy_data(farrel_traditional_source_dir, gdrive_traditional_dir)
# copy_data(farrel_modern_source_dir, gdrive_modern_dir)

# copy_data(dosen_traditional_source_dir, gdrive_traditional_dir)
# copy_data(dosen_modern_source_dir, gdrive_modern_dir)

# copy_data(riqqah_traditional_source_dir, gdrive_traditional_dir)
# copy_data(riqqah_modern_source_dir, gdrive_modern_dir)

# print("All")
# # 10
# # 3
# all_traditional_source_dir = '/content/gdrive/Shared drives/Capstone Project/Dataset/technique-based dataset/all/traditional/'
# all_modern_source_dir = '/content/gdrive/Shared drives/Capstone Project/Dataset/technique-based dataset/all/modern/'
# print('Traditional Batik:', len(os.listdir(all_traditional_source_dir)))
# print('Modern Batik:', len(os.listdir(all_modern_source_dir)))

# print("All except 2")
# # 11
# # 4
# all_traditional_source_dir = '/content/gdrive/Shared drives/Capstone Project/Dataset/technique-based dataset/all_except_2/traditional/'
# all_modern_source_dir = '/content/gdrive/Shared drives/Capstone Project/Dataset/technique-based dataset/all_except_2/modern/'
# print('Traditional Batik:', len(os.listdir(all_traditional_source_dir)))
# print('Modern Batik:', len(os.listdir(all_modern_source_dir)))

# print("All except 3")
# # 10
# # 3
# all_traditional_source_dir = '/content/gdrive/Shared drives/Capstone Project/Dataset/technique-based dataset/all_except_3/traditional/'
# all_modern_source_dir = '/content/gdrive/Shared drives/Capstone Project/Dataset/technique-based dataset/all_except_3/modern/'
# print('Traditional Batik:', len(os.listdir(all_traditional_source_dir)))
# print('Modern Batik:', len(os.listdir(all_modern_source_dir)))
# # # print(os.listdir(all_modern_source_dir))

print("Dataset")
all_traditional_source_dir = '/content/gdrive/Shared drives/Capstone Project/Machine Learning/Dataset/Main Dataset/Batik Technique Dataset/traditional/'
all_modern_source_dir = '/content/gdrive/Shared drives/Capstone Project/Machine Learning/Dataset/Main Dataset/Batik Technique Dataset/modern/'
print('Total:', len(os.listdir(all_traditional_source_dir)) + len(os.listdir(all_modern_source_dir)))
print('Batik Tulis:', len(os.listdir(all_traditional_source_dir)))
print('Batik Cetak:', len(os.listdir(all_modern_source_dir)))

try:
    os.mkdir('/tmp/batik-technique')
    os.mkdir(os.path.join('/tmp/batik-technique/', 'training'))
    os.mkdir(os.path.join('/tmp/batik-technique/', 'validation'))
    os.mkdir(os.path.join('/tmp/batik-technique/', 'testing'))
    os.mkdir(os.path.join('/tmp/batik-technique/training/', 'traditional'))
    os.mkdir(os.path.join('/tmp/batik-technique/training/', 'modern'))
    os.mkdir(os.path.join('/tmp/batik-technique/validation/', 'traditional'))
    os.mkdir(os.path.join('/tmp/batik-technique/validation/', 'modern'))
    os.mkdir(os.path.join('/tmp/batik-technique/testing/', 'traditional'))
    os.mkdir(os.path.join('/tmp/batik-technique/testing/', 'modern'))
except OSError:
    pass

# training_dir = '/tmp/batik-technique/training/'
# training_traditional_dir = '/tmp/batik-technique/training/traditional/'
# training_modern_dir = '/tmp/batik-technique/training/modern/'

# validation_dir = '/tmp/batik-technique/validation/'
# validation_traditional_dir = '/tmp/batik-technique/validation/traditional/'
# validation_modern_dir = '/tmp/batik-technique/validation/modern/'

# testing_dir = '/tmp/batik-technique/testing/'
# testing_traditional_dir = '/tmp/batik-technique/testing/traditional/'
# testing_modern_dir = '/tmp/batik-technique/testing/modern/'


training_dir = "/content/gdrive/Shared drives/Capstone Project/Machine Learning/Dataset/Main Dataset/Batik Technique Dataset Splited/Train/"
training_traditional_dir = "/content/gdrive/Shared drives/Capstone Project/Machine Learning/Dataset/Main Dataset/Batik Technique Dataset Splited/Train/Batik Tulis/"
training_modern_dir = "/content/gdrive/Shared drives/Capstone Project/Machine Learning/Dataset/Main Dataset/Batik Technique Dataset Splited/Train/Batik Cetak/"

validation_dir = "/content/gdrive/Shared drives/Capstone Project/Machine Learning/Dataset/Main Dataset/Batik Technique Dataset Splited/Validation/"
validation_traditional_dir = "/content/gdrive/Shared drives/Capstone Project/Machine Learning/Dataset/Main Dataset/Batik Technique Dataset Splited/Validation/Batik Tulis/"
validation_modern_dir = "/content/gdrive/Shared drives/Capstone Project/Machine Learning/Dataset/Main Dataset/Batik Technique Dataset Splited/Validation/Batik Cetak/"

testing_dir = "/content/gdrive/Shared drives/Capstone Project/Machine Learning/Dataset/Main Dataset/Batik Technique Dataset Splited/Test/"
testing_traditional_dir = "/content/gdrive/Shared drives/Capstone Project/Machine Learning/Dataset/Main Dataset/Batik Technique Dataset Splited/Test/Batik Tulis/"
testing_modern_dir = "/content/gdrive/Shared drives/Capstone Project/Machine Learning/Dataset/Main Dataset/Batik Technique Dataset Splited/Test/Batik Cetak/"

# def split_data(SOURCE, TRAINING, VALIDATION, TESTING, SPLIT_SIZE):
#   if not os.path.exists(TRAINING): 
#     os.makedirs(TRAINING) 

#   if not os.path.exists(VALIDATION): 
#     os.makedirs(VALIDATION)

#   if not os.path.exists(TESTING): 
#     os.makedirs(TESTING) 

#   file_names = os.listdir(SOURCE)

#   random_set = random.sample(file_names, len(file_names))
#   random_set_length = len(random_set)

#   for index, data in enumerate(random_set):
#     if index < SPLIT_SIZE * random_set_length:
#       if index < 0.8 * random_set_length:
#         copyfile(os.path.join(SOURCE, data), os.path.join(TRAINING, data))
#       else:
#         copyfile(os.path.join(SOURCE, data), os.path.join(VALIDATION, data))
#     else: 
#       copyfile(os.path.join(SOURCE, data), os.path.join(TESTING, data))

# split_size = .9
# split_data(all_traditional_source_dir, training_traditional_dir, validation_traditional_dir, testing_traditional_dir, split_size)
# split_data(all_modern_source_dir, training_modern_dir, validation_modern_dir, testing_modern_dir, split_size)

print("Training Data")
print("Train Set:", len(os.listdir(training_traditional_dir)) + len(os.listdir(training_modern_dir)))
print("Batik Tulis:", len(os.listdir(training_traditional_dir)))
print("Batik Cetak :", len(os.listdir(training_modern_dir)))

print("\nValidation Data")
print("Validation Set:", len(os.listdir(validation_traditional_dir)) + len(os.listdir(validation_modern_dir)))
print("Batik Tulis :", len(os.listdir(validation_traditional_dir)))
print("Batik Cetak :", len(os.listdir(validation_modern_dir)))

print("\nTesting Data")
print("Test Set:", len(os.listdir(testing_traditional_dir)) + len(os.listdir(testing_modern_dir)))
print("Batik Tulis :", len(os.listdir(testing_traditional_dir)))
print("Batik Cetak :", len(os.listdir(testing_modern_dir)))

train_batiktulis_names = os.listdir(training_traditional_dir)
print(train_batiktulis_names[:10])

train_batikcetak_names = os.listdir(training_modern_dir)
print(train_batikcetak_names[:10])

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import matplotlib.pyplot as plt
import matplotlib.image as mpimg

# Parameters for our graph; we'll output images in a 4x4 configuration
nrows = 4
ncols = 4

# Index for iterating over images
pic_index = 0

# Set up matplotlib fig, and size it to fit 4x4 pics
fig = plt.gcf()
fig.set_size_inches(ncols * 4, nrows * 4)

pic_index += 8
next_tulis_pix = [os.path.join(training_traditional_dir, fname) 
                for fname in train_batiktulis_names[pic_index-8:pic_index]]
next_cetak_pix = [os.path.join(training_modern_dir, fname) 
                for fname in train_batikcetak_names[pic_index-8:pic_index]]

for i, img_path in enumerate(next_tulis_pix+next_cetak_pix):
  # Set up subplot; subplot indices start at 1
  sp = plt.subplot(nrows, ncols, i + 1)
  sp.axis('Off') # Don't show axes (or gridlines)

  img = mpimg.imread(img_path)
  plt.imshow(img)

plt.show()

# gdrive_train_traditional_path = "/content/gdrive/Shared drives/Capstone Project/Dataset/Main Dataset/Batik Technique Dataset Splited/Train/Batik Tulis/"
# gdrive_train_modern_path = "/content/gdrive/Shared drives/Capstone Project/Dataset/Main Dataset/Batik Technique Dataset Splited/Train/Batik Cetak/"

# gdrive_validation_traditional_path = "/content/gdrive/Shared drives/Capstone Project/Dataset/Main Dataset/Batik Technique Dataset Splited/Validation/Batik Tulis/"
# gdrive_validation_modern_path = "/content/gdrive/Shared drives/Capstone Project/Dataset/Main Dataset/Batik Technique Dataset Splited/Validation/Batik Cetak/"

# gdrive_test_traditional_path = "/content/gdrive/Shared drives/Capstone Project/Dataset/Main Dataset/Batik Technique Dataset Splited/Test/Batik Tulis/"
# gdrive_test_modern_path = "/content/gdrive/Shared drives/Capstone Project/Dataset/Main Dataset/Batik Technique Dataset Splited/Test/Batik Cetak/"

# def copy_data(SOURCE, DESTINATION):
#   file_names = os.listdir(SOURCE)

#   for index, file in enumerate(file_names):
#     copyfile(os.path.join(SOURCE, file), os.path.join(DESTINATION, file))

# # print(os.listdir(modern_test_data_source_path))

# copy_data(training_traditional_dir, gdrive_train_traditional_path)
# copy_data(training_modern_dir, gdrive_train_modern_path)

# copy_data(validation_traditional_dir, gdrive_validation_traditional_path)
# copy_data(validation_modern_dir, gdrive_validation_modern_path)

# copy_data(testing_traditional_dir, gdrive_test_traditional_path)
# copy_data(testing_modern_dir, gdrive_test_modern_path)

train_datagen = ImageDataGenerator(
    rescale = 1/255,
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

train_generator = train_datagen.flow_from_directory(
    training_dir,
    batch_size=11,
    class_mode = 'categorical',
    target_size = (300, 300),
    shuffle=True
)



validation_datagen = ImageDataGenerator(
    rescale = 1/255
)

validation_generator = validation_datagen.flow_from_directory(
    validation_dir,
    batch_size=4,
    class_mode = 'categorical',
    target_size = (300, 300),
    shuffle=True
)



test_datagen = ImageDataGenerator(
    rescale = 1/255
)

test_generator = test_datagen.flow_from_directory(
    testing_dir,
    batch_size=4,
    class_mode = 'categorical',
    target_size = (300, 300),
    shuffle=True
)

base_model = MobileNetV2(
    weights='imagenet',  # Load weights pre-trained on ImageNet.
    input_shape=(300, 300, 3),
    include_top=False
  )

base_model.trainable = False
# for layer in base_model.layers:
#   layer.trainable = False

base_model.summary()

inputs = tf.keras.Input(shape=(300, 300, 3))

x = base_model(inputs, training=False)
x = Flatten()(x)
x = Dense(1024, activation='relu')(x)
x = Dropout(0.2)(x)
outputs = Dense(2, activation='softmax')(x)

model = Model(inputs, outputs)

model.summary()

model.compile(optimizer=RMSprop(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['acc'])

history = model.fit(train_generator,
                    epochs=25,
                    validation_data=validation_generator,
                    verbose = 1)

#----------------------------------------------------------
# Retrieve a list of list results on training and test data
# sets for each training epoch
#-----------------------------------------------------------
acc      = history.history['acc']
val_acc  = history.history['val_acc']
loss     = history.history['loss']
val_loss = history.history['val_loss']

epochs   = range(len(acc)) # Get number of epochs

#------------------------------------------------
# Plot training and validation accuracy per epoch
#------------------------------------------------
plt.plot(epochs, acc, 'r', label='Training accuracy')
plt.plot  ( epochs, val_acc, 'b', label='Validation accuracy')
plt.title ('Training and validation accuracy')
# plt.title('Training accuracy')
plt.legend()
plt.figure()

#------------------------------------------------
# Plot training and validation loss per epoch
#------------------------------------------------
plt.plot(epochs, loss, 'r', label='Training loss')
plt.plot  ( epochs, val_loss, 'b', label='Validation loss')
plt.title ('Training and validation loss')
# plt.title('Training loss')
plt.legend()

plt.show()

labels = train_generator.class_indices.keys()
print(labels)

count_modern = 0
count_traditional = 0
def test_model(source_path, test_set):
  test_set_length = len(test_set)
  count_modern = 0
  count_traditional = 0
  for file_name in test_set:
    img_path = source_path + file_name
    # print(img_path)
    img = image.load_img(img_path, target_size=(300, 300))
    x = image.img_to_array(img)
    x = np.expand_dims(x, axis=0)
    x = preprocess_input(x)

    images = np.vstack([x])
    gdrive_saved_model_path = "/content/gdrive/Shared drives/Capstone Project/Machine Learning/batik_authenticity_classification.h5"
    model = tf.keras.models.load_model(gdrive_saved_model_path)
    result = model.predict(images)[0]
    # print(result[0])
    if result[0] * 100 > 50:
      count_modern += 1
    else:
      count_traditional += 1

    # plt.imshow(img)
    # plt.show()
    
    print("")
    print(file_name)
    for (label, percentage) in zip(labels, result):
      print("{}: {:.2f}%".format(label, percentage*100))

  print("\nJumlah Test Data :", test_set_length)
  print("Terklasifikasi Batik Cetak :", count_modern)
  print("Terklasifikasi Batik Tulis :", count_traditional)


modern_batik_set = os.listdir(testing_modern_dir)
traditional_batik_set = os.listdir(testing_traditional_dir)

# test_model(testing_modern_dir, modern_batik_set)
# test_model(testing_traditional_dir, traditional_batik_set)

# Test Batik Cetak
test_model(testing_modern_dir, modern_batik_set)

# Test Batik Tulis
test_model(testing_traditional_dir, traditional_batik_set)

# labels = train_generator.class_indices.keys()

# uploaded = files.upload()

# for file_name in uploaded.keys():
 
#   # predicting images
#   path = file_name
#   img = image.load_img(path, target_size=(300, 300))
#   x = image.img_to_array(img)
#   x = np.expand_dims(x, axis=0)
#   x = preprocess_input(x)

#   images = np.vstack([x])
#   proba = model.predict(images)[0]
#   plt.imshow(img)
#   plt.show()
#   for (label, p) in zip(labels, proba):
#     print("{}: {:.2f}%".format(label, p * 100))

# gdrive_saved_model_path = "/content/gdrive/Shared drives/Capstone Project/Dataset/technique-based dataset/all/batik_technique_classification_model.h5"
# gdrive_saved_model_path = "/content/gdrive/Shared drives/Capstone Project/Machine Learning/batik_authenticity_classification.h5"

saved_model_path = "/tmp/saved_model/batik_authenticity_classification.h5"

model.save(saved_model_path)

gdrive_saved_model_path = "/content/gdrive/Shared drives/Capstone Project/Machine Learning/batik_authenticity_classification.h5"

copyfile(saved_model_path, gdrive_saved_model_path)

# modern_test_data_source_path = "/tmp/batik-technique/testing/modern/"
# modern_test_data_destination_path = "/content/gdrive/Shared drives/Capstone Project/Dataset/technique-based dataset/all_except_3/test1/modern/"

traditional_test_data_source_path = "/tmp/batik-technique/testing/traditional/"
traditional_test_data_destination_path = "/content/gdrive/Shared drives/Capstone Project/Dataset/technique-based dataset/all_except_3/test1/traditional/"

def copy_data(SOURCE, DESTINATION):
  file_names = os.listdir(SOURCE)

  for index, file in enumerate(file_names):
    copyfile(os.path.join(SOURCE, file), os.path.join(DESTINATION, file))

# print(os.listdir(modern_test_data_source_path))

# copy_data(modern_test_data_source_path, modern_test_data_destination_path)
copy_data(traditional_test_data_source_path, traditional_test_data_destination_path)

mymodel = tf.keras.models.load_model(gdrive_saved_model_path)
print(mymodel)

num_of_test_samples = 56
batch_size = 4
Y_pred = mymodel.predict_generator(test_generator, num_of_test_samples // batch_size + 1)
y_pred = np.argmax(Y_pred, axis=1)

from sklearn.metrics import classification_report, confusion_matrix

print('Confusion Matrix')
print(confusion_matrix(test_generator.classes, y_pred))

print('\nClassification Report')
target_names = ['Batik Cetak', 'Batik Tulis']
print(classification_report(test_generator.classes, y_pred, target_names=target_names))

